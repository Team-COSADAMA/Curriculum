---
title: '데이터 저장하기'
slug: 5-3
category: '5. 크롤링 데이터 다루기'
---

spider가 완성됐어요. 이제 Item의 데이터를 저장할 차례입니다.

```powershell
#명령어 - 터미널에 입력
scrapy crawl <spider 이름> -o <저장할 파일 이름> -t <저장할 파일 형식>
```

spider 실행 명령어에 **-o <저장할 파일 이름>** 과 **-t <저장할 파일 형식>** 을 덧붙이면 될<span style="background-color:#D9E5FF">거예요</span>

```powershell
# 예
scrapy crawl st11_all -o st11_all.csv -t csv      # CSV 형식
scrapy crawl st11_all -o st11_all.json -t json    # JSON 형식
```

📖 **[참고] 저장 가능한 형식**

| 확장자        | 형식 설명                                           |
| ------------- | --------------------------------------------------- |
| json          | JSON 형식의 배열                                    |
| jl(jsonlines) | JSON Lines 형식(줄마다 JSON 객체가 들어있는 텍스트) |
| csv           | CSV 형식                                            |
| xml           | XML 형식                                            |
| marshal       | marshal 모듈로 직렬화한 형태의 바이너리 파일        |
| pickle        | pickle 모듈로 직렬화한 형태의 바이너리 파일         |

CSV 형식으로 저장하는 명령어를 실행해봅시다.

```powershell
#명령어 - 터미널에 입력
scrapy crawl st11_all -o st11_all.csv -t csv
```

![16](/scrapy/5-3/16.png)

프로젝트 디렉토리에 **st11_all.csv** 파일이 생성된 것을 확인할 수 있습니다.

그런데 해당 파일을 열어보니 한글이 깨져 나옵니다 🤖

![17](/scrapy/5-3/17.png)

이러한 한글깨짐 현상은 **settings.py**의 설정을 통해 방지할 수 있습니다.

```python
#settings.py
FEED_EXPORT_ENCODING = 'utf-8'

# 그래도 한글이 깨져 나온다면
FEED_EXPORT_ENCODING = 'cp949'
```

![18](/scrapy/5-3/18.png)

기존 파일을 삭제한 뒤 명령어를 다시 실행해봅시다. (파일 삭제 필수. 덮어쓰기 안 됨.)

```powershell
scrapy crawl st11_all -o st11_all.csv -t csv
```

![19](/scrapy/5-3/19.png)

한글깨짐 없이 데이터가 정상적으로 저장되는 것을 확인할 수 있습니다.
